{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14e06f675ed2c8c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"text-align: center; font-family: 'News Cycle'\">Real-Time Object Detection with YOLO and RealSense Depth Camera</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b05f0e49435811",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Introduction</h2>\n",
    "<p style=\"text-align: justify\">This project implements real-time object detection and segmentation using the YOLO (You Only Look Once) model, integrated with a RealSense camera. The script captures video frames from the RealSense camera applies object detection, overlays segmentation masks, and visualizes the results in real-time.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf17607de7f8284",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3>Object Detection</h3>\n",
    "<table width=\"100%\">\n",
    "    <tr>\n",
    "        <td width=\"50%\" bgcolor=\"#ffffff\">\n",
    "            <p style=\"text-align: justify\">Object detection is a computer vision technique that identifies and locates objects within an image or video frame. This process involves both classifying objects and determining their positions, typically marked with bounding boxes.</p>\n",
    "        </td>\n",
    "        <td width=\"50%\" bgcolor=\"#ffffff\">\n",
    "            <img src=\"img/ObjDet.jpg\" alt=\"RealSense D415 Depth Camera\" />\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<br></br>\n",
    "<hr>\n",
    "<footer>\n",
    "    <p style=\"text-align: right\" ><a href=\"https://www.freepik.com/free-vector/businessman-technology-measuring-eye-position-movement-tiny-people-eye-tracking-technology-gaze-tracking-eye-position-sensor-concept_11669265.htm#query=object%20recognition&position=20&from_view=keyword&track=ais&uuid=ea96fe7c-a24b-456f-b3da-470e6a4a764e#position=20&query=object%20recognition\">Image source: freepik.com</a></p>\n",
    "</footer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cdaf4d2320c25",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3>RealSense</h3>\n",
    "<table width=\"100%\">\n",
    "    <tr>\n",
    "        <td width=\"50%\" bgcolor=\"#ffffff\">\n",
    "            <p style=\"text-align: justify\">The RealSense camera is a series of depth-sensing cameras developed by Intel, designed to capture 3D spatial data and enable depth perception in various applications, ranging from virtual reality and augmented reality to robotics and gesture recognition.</p>\n",
    "        </td>\n",
    "        <td width=\"50%\" bgcolor=\"#ffffff\">\n",
    "            <img src=\"img/d415_front.png\" alt=\"RealSense D415 Depth Camera\" />\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<hr>\n",
    "<footer>\n",
    "    <p style=\"text-align: right\"><a href=\"https://www.intelrealsense.com/depth-camera-d415/\">Source: intelrealsense.com</a></p>\n",
    "</footer>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36222b769b35aa41",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3>Industrial Use Cases</h3>\n",
    "<table width=\"100%\">\n",
    "    <tr>\n",
    "        <td width=\"60%\" bgcolor=\"#ffffff\">\n",
    "            <ul style=\"text-align: left\">\n",
    "                <li>Quality Control</li>\n",
    "                <li>Inventory Management</li>\n",
    "                <li>Safety Compliance Monitoring</li>\n",
    "                <li>Automation in Manufacturing</li>\n",
    "                <li>Robotics and Automated Guided Vehicles (AGVs)</li>\n",
    "                <li>Agricultural Automation</li>\n",
    "                <li>Surveillance and Security</li>\n",
    "                <li>Pharmaceuticals and Healthcare</li>\n",
    "                <li>Food and Beverage Industry</li>\n",
    "                <li>Mining and Construction</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "        <td width=\"40%\" bgcolor=\"#ffffff\">\n",
    "            <img src=\"img/industry.jpg\" alt=\"RealSense D415 Depth Camera\" />\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<hr>\n",
    "<footer>\n",
    "    <p style=\"text-align: right\"><a href=\"https://www.freepik.com/free-vector/factory-workers-robotic-arm-removing-packages-from-conveyor-line-engineer-using-computer-operating-process-vector-illustration-business-production-machine-technology-concepts_10606454.htm#query=industry&position=13&from_view=search&track=sph&uuid=3a113fa2-d3f6-4ce3-8a5b-72f1688e2c5c\">Image source: freepik.com</a></p>\n",
    "</footer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4990fcf07305fd06",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Machine Learning Models</h2>\n",
    "<p style=\"text-align: justify\">Machine learning object detection models identify and locate objects in images or videos. They use algorithms, typically based on convolutional neural networks, to classify objects and pinpoint their positions with bounding boxes. Popular models like YOLO, SSD, and Faster R-CNN vary in speed and accuracy, and are essential in applications like autonomous driving, surveillance, and augmented reality.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610479d5445118b9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3>Model Comparison</h3>\n",
    "<table width=\"100%\">\n",
    "    <tr>\n",
    "        <td width=\"60%\" bgcolor=\"#ffffff\">\n",
    "            <p style=\"text-align: justify\">Faster RCNN, YOLO and SSD are three popular object detection systems that use deep learning to locate and classify objects in images. They differ in their architectures, speed and accuracy. Here is a brief comparison of their main features:</p>\n",
    "        </td>\n",
    "        <td width=\"40%\" bgcolor=\"#ffffff\">\n",
    "            <img src=\"img/compare.jpg\" alt=\"RealSense D415 Depth Camera\" />\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<hr>\n",
    "<footer>\n",
    "    <p style=\"text-align: right\"><a href=\"https://www.freepik.com/free-vector/brainstorming-abstract-concept_12084818.htm#page=9&position=28&from_view=author&uuid=4dfa2bda-495e-49fd-a4aa-60583a8ef1fd\">Image source: freepik.com</a></p>\n",
    "</footer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3703243fe258f6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table width=\"100%\">\n",
    "    <tr>\n",
    "        <td width=\"70%\" bgcolor=\"#ffffff\" style=\"text-align: left\">\n",
    "            <dl>\n",
    "              <dt>Faster RCNN:</dt>\n",
    "              <dd style=\"text-align: justify\">This system consists of two modules: a region proposal network (RPN) that generates candidate regions of interest (RoIs), and a Fast RCNN network that classifies and refines the RoIs. Faster RCNN is accurate and robust, but it is slow compared to the other two systems, as it requires multiple stages and computations.</dd>\n",
    "              <dt>YOLO:</dt>\n",
    "              <dd style=\"text-align: justify\">This system divides the input image into a grid of cells, and predicts bounding boxes and class probabilities for each cell. YOLO is fast and efficient, as it performs object detection in a single pass through the network. However, it may struggle with small or overlapping objects, as it has a limited number of bounding boxes per cell.</dd>\n",
    "              <dt>SSD:</dt>\n",
    "              <dd style=\"text-align: justify\">This system also performs object detection in a single pass, but it uses multiple feature maps of different resolutions to generate bounding boxes and class probabilities. SSD is faster than Faster RCNN and more accurate than YOLO, as it can detect objects of various sizes and shapes. However, it may still miss some small or occluded objects, as it relies on fixed aspect ratios and scales.</dd>\n",
    "            </dl>\n",
    "        </td>\n",
    "        <td width=\"30%\" bgcolor=\"#ffffff\" style=\"text-align: left\">\n",
    "            <table width=\"100%\">\n",
    "                <tr>\n",
    "                    <th width=\"30%\" bgcolor=\"#ffffff\" style=\"text-align: right\">Faster RCNN:</th>\n",
    "                    <td width=\"70%\" bgcolor=\"#ffffff\">\n",
    "                    <p><meter style=\"width: 100%\" min=\"0\" max=\"10\" low=\"3\" high=\"10\" optimum=\"10\" value=\"5\"></meter></p>\n",
    "                    <p><meter style=\"width: 100%\" min=\"0\" max=\"10\" low=\"5\" high=\"10\" optimum=\"10\" value=\"10\"></meter></p>\n",
    "                    </td>\n",
    "                </tr>\n",
    "                <tr style=\"height: 10px\" bgcolor=\"#ffffff\">\n",
    "                    <td bgcolor=\"#ffffff\"></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th width=\"30%\" bgcolor=\"#ffffff\" style=\"text-align: right\">YOLO:</th>\n",
    "                    <td width=\"70%\" bgcolor=\"#ffffff\">\n",
    "                    <p><meter style=\"width: 100%\" min=\"0\" max=\"10\" low=\"5\" high=\"10\" optimum=\"10\" value=\"10\"></meter></p>\n",
    "                    <p><meter style=\"width: 100%\" min=\"0\" max=\"10\" low=\"5\" high=\"10\" optimum=\"10\" value=\"5\"></meter></p>\n",
    "                    </td>\n",
    "                </tr>\n",
    "                <tr style=\"height: 10px\" bgcolor=\"#ffffff\">\n",
    "                    <td bgcolor=\"#ffffff\"></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th width=\"30%\" bgcolor=\"#ffffff\" style=\"text-align: right\">SSD:</th>\n",
    "                    <td width=\"70%\" bgcolor=\"#ffffff\">\n",
    "                    <p><meter style=\"width: 100%\" min=\"0\" max=\"10\" low=\"5\" high=\"10\" optimum=\"10\" value=\"8\"></meter></p>\n",
    "                    <p><meter style=\"width: 100%\" min=\"0\" max=\"10\" low=\"5\" high=\"10\" optimum=\"10\" value=\"8\"></meter></p>\n",
    "                    </td>\n",
    "                </tr>\n",
    "            </table>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<br></br>\n",
    "<hr>\n",
    "<footer>\n",
    "<p style=\"text-align: right\"><a href=\"https://medium.com/ibm-data-ai/faster-r-cnn-vs-yolo-vs-ssd-object-detection-algorithms-18badb0e02dc\">Source: medium.com</a></p>\n",
    "</footer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6525a3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p style=\"text-align: justify\">In summary, Faster RCNN is suitable for applications that require high accuracy and can tolerate low speed, such as medical image analysis or autonomous driving. YOLO is suitable for applications that require real-time performance and can tolerate some errors, such as video surveillance or sports analysis. SSD is a good compromise between speed and accuracy, and can be used for general-purpose object detection tasks.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd27fe27990ca6e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3>YOLO v8</h3>\n",
    "<table width=\"100%\">\n",
    "    <tr>\n",
    "        <td width=\"50%\" bgcolor=\"#ffffff\">\n",
    "            <p style=\"text-align: justify\">YOLOv8 is the latest version of YOLO by Ultralytics. As a cutting-edge, state-of-the-art (SOTA) model, YOLOv8 builds on the success of previous versions, introducing new features and improvements for enhanced performance, flexibility, and efficiency. YOLOv8 supports a full range of vision AI tasks, including detection, segmentation, pose estimation, tracking, and classification. This versatility allows users to leverage YOLOv8's capabilities across diverse applications and domains.</p>\n",
    "        </td>\n",
    "        <td width=\"50%\" bgcolor=\"#ffffff\" >\n",
    "            <img src=\"img/ultralytics.svg\" alt=\"RealSense D415 Depth Camera\" align=\"right\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<hr>\n",
    "<footer>\n",
    "    <p style=\"text-align: right\"><a href=\"https://www.ultralytics.com/brand\">Image source: ultralytics.com</a>&nbsp;&nbsp;<a href=\"https://docs.ultralytics.com/\">Source: ultralytics.com</a></p>\n",
    "</footer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9a1fc7b69cdf6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Different functionalities\n",
    "dfasgasdfgsdfgsd dfsgkj hsdlg sldifug sjg sldfkjfgh lskjdjghf lksjdfhg lka glijdjg hlkjjgfh lkfdgh\n",
    "sldfjkgh sdf g;oddg dfjgh ;klafhgj;kahfg; afg adfg kajgh ;akljfgh;aookdfgh afkjg akjgfh\n",
    "pioadfhg pag ;aog ;aoishjgf poasdfj ipoahj g;a gjf;aois gj;oaiisjgd ;aosigd hjasdg\n",
    "aspoiigdh paog paoig jao;s gj[aoiugd poaish g;kajshdg;kang;lkashg j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea6adbc6fe57b6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Training\n",
    "dfasgasdfgsdfgsd dfsgkj hsdlg sldifug sjg sldfkjfgh lskjdjghf lksjdfhg lka glijdjg hlkjjgfh lkfdgh\n",
    "sldfjkgh sdf g;oddg dfjgh ;klafhgj;kahfg; afg adfg kajgh ;akljfgh;aookdfgh afkjg akjgfh\n",
    "pioadfhg pag ;aog ;aoishjgf poasdfj ipoahj g;a gjf;aois gj;oaiisjgd ;aosigd hjasdg\n",
    "aspoiigdh paog paoig jao;s gj[aoiugd poaish g;kajshdg;kang;lkashg j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f904c277e025716d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### COCO training set\n",
    "dfasgasdfgsdfgsd dfsgkj hsdlg sldifug sjg sldfkjfgh lskjdjghf lksjdfhg lka glijdjg hlkjjgfh lkfdgh\n",
    "sldfjkgh sdf g;oddg dfjgh ;klafhgj;kahfg; afg adfg kajgh ;akljfgh;aookdfgh afkjg akjgfh\n",
    "pioadfhg pag ;aog ;aoishjgf poasdfj ipoahj g;a gjf;aois gj;oaiisjgd ;aosigd hjasdg\n",
    "aspoiigdh paog paoig jao;s gj[aoiugd poaish g;kajshdg;kang;lkashg j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8561d01855b5b7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Pretrained models\n",
    "dfasgasdfgsdfgsd dfsgkj hsdlg sldifug sjg sldfkjfgh lskjdjghf lksjdfhg lka glijdjg hlkjjgfh lkfdgh\n",
    "sldfjkgh sdf g;oddg dfjgh ;klafhgj;kahfg; afg adfg kajgh ;akljfgh;aookdfgh afkjg akjgfh\n",
    "pioadfhg pag ;aog ;aoishjgf poasdfj ipoahj g;a gjf;aois gj;oaiisjgd ;aosigd hjasdg\n",
    "aspoiigdh paog paoig jao;s gj[aoiugd poaish g;kajshdg;kang;lkashg j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d91bef607d262c0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a060b5beccbdbb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Implementation</h2>\n",
    "<table width=\"100%\">\n",
    "    <tr>\n",
    "        <td width=\"50%\" bgcolor=\"#ffffff\">\n",
    "            <p style=\"text-align: justify\">The integration of the RealSense depth camera and the YOLOv8 system is mainly achieved through a continuous feed processing loop. However, some preparation is necessary. In this section, the code is broken up into chunks to ease the readability and provide better commentary. As shown in the diagram, the main loop is charachterized with a flowchart.</p>\n",
    "        </td>\n",
    "        <td style=\"width: 100px\" bgcolor=\"#ffffff\">\n",
    "        </td>\n",
    "        <td width=\"50%\" bgcolor=\"#ffffff\" >\n",
    "            <img src=\"img/diag.png\" alt=\"Main loop flowchart\" align=\"right\" />\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d258b6801581793",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3>Prerequisites</h3>\n",
    "<p style=\"text-align: justify\">Libraries used throughout the code is imported here.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615cbc9d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475fc29d5a390c79",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3>Overlay Function</h3>\n",
    "<p style=\"text-align: justify\">This function is in charge of overlaying a generated mask with its coresponding colour on top of the input image.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918b9f3b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def overlay(image, mask, color, alpha, resize=None):\n",
    "    colored_mask = np.expand_dims(mask, 0).repeat(3, axis=0)\n",
    "    colored_mask = np.moveaxis(colored_mask, 0, -1)\n",
    "    masked = np.ma.MaskedArray(image, mask=colored_mask, fill_value=color)\n",
    "    image_overlay = masked.filled()\n",
    "\n",
    "    if resize is not None:\n",
    "        image = cv2.resize(image.transpose(1, 2, 0), resize)\n",
    "        image_overlay = cv2.resize(image_overlay.transpose(1, 2, 0), resize)\n",
    "        \n",
    "    image_combined = cv2.addWeighted(image, 1 - alpha, image_overlay, alpha, 0)\n",
    "    return image_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daf17b3767eafb8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3>Plot Box Function</h3>\n",
    "<p style=\"text-align: justify\">This function plots a rectangle around a detected object while displaying the classification category, porbability, and the measured distance.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd4e97de",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def plot_one_box(x, img, color=None, label=None, line_thickness=3):\n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    if label:\n",
    "        tf = max(tl - 1, 1)\n",
    "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)\n",
    "        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9858fbb351c22d2a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3>Camera Feed Preparation</h3>\n",
    "<p style=\"text-align: justify\">To access the color and depth images of the RealSense camera, a pipeline is defined, configured and started. It is also necessary to mention that an image alignment is also performed here so the positional data extracted from each fram is interchangable.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbe5e89",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, 1280, 720, rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)\n",
    "align = rs.align(rs.stream.depth)\n",
    "\n",
    "print(\"[INFO] Starting streaming...\")\n",
    "pipeline.start(config)\n",
    "print(\"[INFO] Camera ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec5ecdd69eee8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3>Loading Model</h3>\n",
    "<p style=\"text-align: justify\">Here, a model instance is initiated and a nano YOLOv8 segmentation model is stored. List of detectable objects is also extracted and printed to console.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e702b7c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8n-seg.pt\")\n",
    "class_names = model.names\n",
    "print('Class Names: ', class_names)\n",
    "colors = [[random.randint(0, 255) for _ in range(3)] for _ in class_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfdabb7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3>Main Loop</h3>\n",
    "<p style=\"text-align: justify\">With the necessary preparations made, an infinite loop an be constructed. Within each itteration, a color and depth frame pair is prcessed and displayed.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df833292",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "frames = pipeline.wait_for_frames()\n",
    "aligned_frames = align.process(frames)\n",
    "depth_frame = aligned_frames.get_depth_frame()\n",
    "aligned_color_frame = aligned_frames.get_color_frame()\n",
    "color_frame = frames.get_color_frame()\n",
    "if not depth_frame or not aligned_color_frame: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538d05de",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "color_intrin = aligned_color_frame.profile.as_video_stream_profile().intrinsics\n",
    "depth_image = np.asanyarray(depth_frame.get_data())\n",
    "color_image = np.asanyarray(color_frame.get_data())\n",
    "h, w, _ = color_image.shape\n",
    "results = model.predict(color_image, stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e95c3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for r in results:\n",
    "        boxes = r.boxes  # Boxes object for bbox outputs\n",
    "        masks = r.masks  # Masks object for segment masks outputs\n",
    "        probs = r.probs  # Class probabilities for classification outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a222bb4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "if masks is not None:\n",
    "    masks = masks.data.cpu()\n",
    "    for seg, box in zip(masks.data.cpu().numpy(), boxes):\n",
    "        seg = cv2.resize(seg, (w, h))\n",
    "        color_image = overlay(color_image, seg, colors[int(box.cls)], 0.4)\n",
    "\n",
    "        count = (seg == 1).sum()\n",
    "        x, y = np.argwhere(seg == 1).sum(0) / count\n",
    "        depth = depth_frame.get_distance(int(y), int(x))\n",
    "        dx, dy, dz = rs.rs2_deproject_pixel_to_point(color_intrin, [y, x], depth)\n",
    "        distance = math.sqrt(((dx) ** 2) + ((dy) ** 2) + ((dz) ** 2))\n",
    "\n",
    "        xmin = int(box.data[0][0])\n",
    "        ymin = int(box.data[0][1])\n",
    "        xmax = int(box.data[0][2])\n",
    "        ymax = int(box.data[0][3])\n",
    "\n",
    "        plot_one_box([xmin, ymin, xmax, ymax], color_image, colors[int(box.cls)],\n",
    "                     f'{class_names[int(box.cls)]} {float(box.conf):.3} {float(100*distance):.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef752f4c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "cv2.imshow('img', color_image)\n",
    "if cv2.waitKey(1) & 0xFF == ord('q'):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c73c21614c289bf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3>Clean-up and Resource Management</h3>\n",
    "<p style=\"text-align: justify\">To free up memory and disengage the camera, pipeline needs to be stopped at the end of operation.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a247a5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"[INFO] stop streaming ...\")\n",
    "pipeline.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dc69f1",
   "metadata": {},
   "source": [
    "<h2>Live Demo</h2>\n",
    "<table width=\"100%\">\n",
    "    <tr>\n",
    "        <td width=\"50%\" bgcolor=\"#ffffff\">\n",
    "            <p style=\"text-align: justify\">The integration of the RealSense depth camera and the YOLOv8 system is mainly achieved through a continuous feed processing loop. However, some preparation is necessary. In this section, the code is broken up into chunks to ease the readability and provide better commentary. As shown in the diagram, the main loop is charachterized with a flowchart.</p>\n",
    "        </td>\n",
    "        <td style=\"width: 100px\" bgcolor=\"#ffffff\">\n",
    "        </td>\n",
    "        <td width=\"50%\" bgcolor=\"#ffffff\" >\n",
    "            <img src=\"img/diag.png\" alt=\"Main loop flowchart\" align=\"right\" />\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47f22c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
