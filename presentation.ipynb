{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14e06f675ed2c8c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"text-align: center; font-family: 'News Cycle'\">Real-Time Object Detection with YOLO and RealSense Depth Camera</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b05f0e49435811",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h2 style=\"font-family: 'News Cycle'\">Introduction</h2>\n",
    "<p style=\"text-align: justify; font-family: 'News Cycle'\" >This project implements real-time object detection and segmentation using the YOLO (You Only Look Once) model, integrated with a RealSense camera. The script captures video frames from the RealSense camera applies object detection, overlays segmentation masks, and visualizes the results in real-time.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf17607de7f8284",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"font-family: 'News Cycle'\">Object Detection</h3>\n",
    "<table width=\"100%\">\n",
    "    <tr>\n",
    "        <td width=\"50%\" bgcolor=\"#ffffff\">\n",
    "            <p style=\"text-align: justify; font-family: 'News Cycle'\">Object detection is a computer vision technique that identifies and locates objects within an image or video frame. This process involves both classifying objects and determining their positions, typically marked with bounding boxes.</p>\n",
    "        </td>\n",
    "        <td width=\"50%\" bgcolor=\"#ffffff\">\n",
    "            <img src=\"img/ObjDet.jpg\" alt=\"RealSense D415 Depth Camera\" />\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cdaf4d2320c25",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"font-family: 'News Cycle'\">RealSense</h3>\n",
    "<table width=\"100%\">\n",
    "    <tr>\n",
    "        <td width=\"50%\" bgcolor=\"#ffffff\">\n",
    "            <p style=\"text-align: justify; font-family: 'News Cycle'\">The RealSense camera is a series of depth-sensing cameras developed by Intel, designed to capture 3D spatial data and enable depth perception in various applications, ranging from virtual reality and augmented reality to robotics and gesture recognition.</p>\n",
    "        </td>\n",
    "        <td width=\"50%\" bgcolor=\"#ffffff\">\n",
    "            <img src=\"img/d415_front.png\" alt=\"RealSense D415 Depth Camera\" />\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36222b769b35aa41",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"font-family: 'News Cycle'\">Industrial Use Cases</h3>\n",
    "<table width=\"100%\">\n",
    "    <tr>\n",
    "        <td width=\"60%\" bgcolor=\"#ffffff\">\n",
    "            <ul style=\"text-align: left; font-family: 'News Cycle'\">\n",
    "                <li>Quality Control</li>\n",
    "                <li>Inventory Management</li>\n",
    "                <li>Safety Compliance Monitoring</li>\n",
    "                <li>Automation in Manufacturing</li>\n",
    "                <li>Robotics and Automated Guided Vehicles (AGVs)</li>\n",
    "                <li>Agricultural Automation</li>\n",
    "                <li>Surveillance and Security</li>\n",
    "                <li>Pharmaceuticals and Healthcare</li>\n",
    "                <li>Food and Beverage Industry</li>\n",
    "                <li>Mining and Construction</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "        <td width=\"40%\" bgcolor=\"#ffffff\">\n",
    "            <img src=\"img/industry.jpg\" alt=\"RealSense D415 Depth Camera\"  style=\"display: block; margin-left: auto; margin-right: auto\"  />\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4990fcf07305fd06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h2 style=\"font-family: 'News Cycle'\">Machine Learning Models</h2>\n",
    "<p style=\"text-align: justify; font-family: 'News Cycle'\">Machine learning object detection models identify and locate objects in images or videos. They use algorithms, typically based on convolutional neural networks, to classify objects and pinpoint their positions with bounding boxes. Popular models like YOLO, SSD, and Faster R-CNN vary in speed and accuracy and are essential in applications like autonomous driving, surveillance, and augmented reality.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610479d5445118b9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"font-family: 'News Cycle'\">Model Comparison</h3>\n",
    "<table width=\"100%\">\n",
    "    <tr>\n",
    "        <td width=\"60%\" bgcolor=\"#ffffff\">\n",
    "            <p style=\"text-align: justify; font-family: 'News Cycle'\">Faster RCNN, YOLO, and SSD are three popular object detection systems that use deep learning to locate and classify objects in images. They differ in their architectures, speed, and accuracy. Here is a brief comparison of their main features:</p>\n",
    "        </td>\n",
    "        <td width=\"40%\" bgcolor=\"#ffffff\">\n",
    "            <img src=\"img/compare.jpg\" alt=\"RealSense D415 Depth Camera\" />\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3703243fe258f6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<table width=\"100%\">\n",
    "    <tr>\n",
    "        <td width=\"70%\" bgcolor=\"#ffffff\" style=\"text-align: left\">\n",
    "            <dl>\n",
    "              <dt>Faster RCNN:</dt>\n",
    "              <dd style=\"text-align: justify; font-family: 'News Cycle'\">This system consists of two modules: a region proposal network (RPN) that generates candidate regions of interest (RoIs), and a Fast RCNN network that classifies and refines the RoIs. Faster RCNN is accurate and robust, but it is slow compared to the other two systems, as it requires multiple stages and computations.</dd>\n",
    "              <dt>YOLO:</dt>\n",
    "              <dd style=\"text-align: justify; font-family: 'News Cycle'\">This system divides the input image into a grid of cells, and predicts bounding boxes and class probabilities for each cell. YOLO is fast and efficient, as it performs object detection in a single pass through the network. However, it may struggle with small or overlapping objects, as it has a limited number of bounding boxes per cell.</dd>\n",
    "              <dt>SSD:</dt>\n",
    "              <dd style=\"text-align: justify; font-family: 'News Cycle'\">This system also performs object detection in a single pass, but it uses multiple feature maps of different resolutions to generate bounding boxes and class probabilities. SSD is faster than Faster RCNN and more accurate than YOLO, as it can detect objects of various sizes and shapes. However, it may still miss some small or occluded objects, as it relies on fixed aspect ratios and scales.</dd>\n",
    "            </dl>\n",
    "        </td>\n",
    "        <td width=\"30%\" bgcolor=\"#ffffff\" style=\"text-align: left; font-family: 'News Cycle'\">\n",
    "            <table width=\"100%\">\n",
    "                <tr>\n",
    "                    <th width=\"30%\" bgcolor=\"#ffffff\" style=\"text-align: right; font-family: 'News Cycle'\">Faster RCNN:</th>\n",
    "                    <td width=\"70%\" bgcolor=\"#ffffff\">\n",
    "                    <p><meter style=\"width: 100%\" min=\"0\" max=\"10\" low=\"3\" high=\"10\" optimum=\"10\" value=\"5\"></meter></p>\n",
    "                    <p><meter style=\"width: 100%\" min=\"0\" max=\"10\" low=\"5\" high=\"10\" optimum=\"10\" value=\"10\"></meter></p>\n",
    "                    </td>\n",
    "                </tr>\n",
    "                <tr style=\"height: 10px\" bgcolor=\"#ffffff\">\n",
    "                    <td bgcolor=\"#ffffff\"></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th width=\"30%\" bgcolor=\"#ffffff\" style=\"text-align: right; font-family: 'News Cycle'\">YOLO:</th>\n",
    "                    <td width=\"70%\" bgcolor=\"#ffffff\">\n",
    "                    <p><meter style=\"width: 100%\" min=\"0\" max=\"10\" low=\"5\" high=\"10\" optimum=\"10\" value=\"10\"></meter></p>\n",
    "                    <p><meter style=\"width: 100%\" min=\"0\" max=\"10\" low=\"5\" high=\"10\" optimum=\"10\" value=\"5\"></meter></p>\n",
    "                    </td>\n",
    "                </tr>\n",
    "                <tr style=\"height: 10px\" bgcolor=\"#ffffff\">\n",
    "                    <td bgcolor=\"#ffffff\"></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th width=\"30%\" bgcolor=\"#ffffff\" style=\"text-align: right; font-family: 'News Cycle'\">SSD:</th>\n",
    "                    <td width=\"70%\" bgcolor=\"#ffffff\">\n",
    "                    <p><meter style=\"width: 100%\" min=\"0\" max=\"10\" low=\"5\" high=\"10\" optimum=\"10\" value=\"8\"></meter></p>\n",
    "                    <p><meter style=\"width: 100%\" min=\"0\" max=\"10\" low=\"5\" high=\"10\" optimum=\"10\" value=\"8\"></meter></p>\n",
    "                    </td>\n",
    "                </tr>\n",
    "            </table>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6525a3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<p style=\"text-align: justify; font-family: 'News Cycle'\">In summary, Faster RCNN is suitable for applications that require high accuracy and can tolerate low speed, such as medical image analysis or autonomous driving. YOLO is suitable for applications that require real-time performance and can tolerate some errors, such as video surveillance or sports analysis. SSD is a good compromise between speed and accuracy and can be used for general-purpose object detection tasks.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd27fe27990ca6e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"font-family: 'News Cycle'\">YOLOv8</h3>\n",
    "<table width=\"100%\">\n",
    "    <tr>\n",
    "        <td width=\"50%\" bgcolor=\"#ffffff\">\n",
    "            <p style=\"text-align: justify; font-family: 'News Cycle'\">YOLOv8 is the latest version of YOLO by Ultralytics. As a cutting-edge, state-of-the-art (SOTA) model, YOLOv8 builds on the success of previous versions, introducing new features and improvements for enhanced performance, flexibility, and efficiency. YOLOv8 supports a full range of vision AI tasks, including detection, segmentation, pose estimation, tracking, and classification. This versatility allows users to leverage YOLOv8's capabilities across diverse applications and domains.</p>\n",
    "        </td>\n",
    "        <td width=\"50%\" bgcolor=\"#ffffff\" >\n",
    "            <img src=\"img/ultralytics.svg\" alt=\"RealSense D415 Depth Camera\" align=\"right\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9a1fc7b69cdf6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<h4 style=\"font-family: 'News Cycle'\">Different functionalities</h4>\n",
    "<img src=\"img/function.png\" alt=\"Different functionalities\"   style=\"display: block; margin-left: auto; margin-right: auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4393ef60-f38e-4ed5-b045-a210ad607c3d",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "Detection\n",
    "Detection is the primary task supported by YOLOv8. It involves detecting objects in an image or video frame and drawing bounding boxes around them. The detected objects are classified into different categories based on their features. YOLOv8 can detect multiple objects in a single image or video frame with high accuracy and speed.\n",
    "\n",
    "Segmentation\n",
    "Segmentation is a task that involves segmenting an image into different regions based on the content of the image. Each region is assigned a label based on its content. This task is useful in applications such as image segmentation and medical imaging. YOLOv8 uses a variant of the U-Net architecture to perform segmentation.\n",
    "\n",
    "Classification\n",
    "Classification is a task that involves classifying an image into different categories. YOLOv8 can be used to classify images based on their content. It uses a variant of the Efficient Net architecture to perform classification.\n",
    "\n",
    "Pose\n",
    "Pose/key point detection is a task that involves detecting specific points in an image or video frame. These points are referred to as key points and are used to track movement or pose estimation. YOLOv8 can detect key points in an image or video frame with high accuracy. and speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea6adbc6fe57b6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<h4 style=\"font-family: 'News Cycle'\">Training</h4>\n",
    "<p style=\"text-align: justify; font-family: 'News Cycle'\">Object detection is a computer vision technique that identifies and locates objects within an image or video frame. This process involves both classifying objects and determining their positions, typically marked with bounding boxes.</p>\n",
    "<img src=\"img/Integrations.png\" alt=\"RealSense D415 Depth Camera\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c5fa8c-0b27-4b6f-bbad-894b0db5670e",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "Training a deep learning model involves feeding it data and adjusting its parameters so that it can make accurate predictions. Train mode in Ultralytics YOLOv8 is engineered for effective and efficient training of object detection models, fully utilizing modern hardware capabilities. This guide aims to cover all the details you need to get started with training your own models using YOLOv8's robust set of features.\n",
    "Key Features of Train Mode\n",
    "The following are some notable features of YOLOv8's Train mode:\n",
    "Automatic Dataset Download: Standard datasets like COCO, VOC, and ImageNet are downloaded automatically on first use.\n",
    "Multi-GPU Support: Scale your training efforts seamlessly across multiple GPUs to expedite the process.\n",
    "Hyperparameter Configuration: The option to modify hyperparameters through YAML configuration files or CLI arguments.\n",
    "Visualization and Monitoring: Real-time tracking of training metrics and visualization of the learning process for better insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f904c277e025716d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<h4 style=\"font-family: 'News Cycle'\">COCO training set</h4>\n",
    "<table width=\"100%\">\n",
    "    <tr>\n",
    "        <td width=\"50%\" bgcolor=\"#ffffff\">\n",
    "            <p style=\"text-align: justify; font-family: 'News Cycle'\">The COCO (Common Objects in Context) dataset is a large-scale collection of images used for object detection, segmentation, and captioning. It features over 200,000 images with detailed annotations for various objects in diverse environments. COCO128 is a smaller subset of the COCO dataset, containing approximately 128 images. It retains the full dataset's diverse scenarios and rich annotations but is more manageable for quick testing, prototyping, and educational purposes due to its smaller size.e.</p>\n",
    "        </td>\n",
    "        <td width=\"50%\" bgcolor=\"#ffffff\">\n",
    "            <img src=\"img/COCO.png\" alt=\"COCO log align=\"right\"o\" />\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a060b5beccbdbb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h2  style=\"font-family: 'News Cycle'\">Implementation</h2>\n",
    "<table width=\"100%\">\n",
    "    <tr>\n",
    "        <td width=\"50%\" bgcolor=\"#ffffff\">\n",
    "            <p style=\"text-align: justify; font-family: 'News Cycle'\">The integration of the RealSense depth camera and the YOLOv8 system is mainly achieved through a continuous feed processing loop. However, some preparation is necessary. In this section, the code is broken up into chunks to ease the readability and provide better commentary. As shown in the diagram, the main loop is characterized by a flowchart.</p>\n",
    "        </td>\n",
    "        <td style=\"width: 100px\" bgcolor=\"#ffffff\">\n",
    "        </td>\n",
    "        <td width=\"50%\" bgcolor=\"#ffffff\" >\n",
    "            <img src=\"img/diag.png\" alt=\"Main loop flowchart\" align=\"right\" />\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d258b6801581793",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<h3  style=\"font-family: 'News Cycle'\">Prerequisites</h3>\n",
    "<p style=\"text-align: justify\">Libraries used throughout the code is imported here.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bfc321",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475fc29d5a390c79",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<h3  style=\"font-family: 'News Cycle'\">Overlay Function</h3>\n",
    "<p style=\"text-align: justify\">This function is in charge of overlaying a generated mask with its corresponding colour on top of the input image.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245379ca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def overlay(image, mask, color, alpha, resize=None):\n",
    "    colored_mask = np.expand_dims(mask, 0).repeat(3, axis=0)\n",
    "    colored_mask = np.moveaxis(colored_mask, 0, -1)\n",
    "    masked = np.ma.MaskedArray(image, mask=colored_mask, fill_value=color)\n",
    "    image_overlay = masked.filled()\n",
    "\n",
    "    if resize is not None:\n",
    "        image = cv2.resize(image.transpose(1, 2, 0), resize)\n",
    "        image_overlay = cv2.resize(image_overlay.transpose(1, 2, 0), resize)\n",
    "        \n",
    "    image_combined = cv2.addWeighted(image, 1 - alpha, image_overlay, alpha, 0)\n",
    "    return image_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daf17b3767eafb8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<h3  style=\"font-family: 'News Cycle'\">Plot Box Function</h3>\n",
    "<p style=\"text-align: justify\">This function plots a rectangle around a detected object while displaying the classification category, probability, and measured distance.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df2c526e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_one_box(x, img, color=None, label=None, line_thickness=3):\n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    if label:\n",
    "        tf = max(tl - 1, 1)\n",
    "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)\n",
    "        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9858fbb351c22d2a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<h3  style=\"font-family: 'News Cycle'\">Camera Feed Preparation</h3>\n",
    "<p style=\"text-align: justify\">To access the color and depth images of the RealSense camera, a pipeline is defined, configured, and started. It is also necessary to mention that an image alignment is also performed here so the positional data extracted from each frame is interchangeable.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3d634a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, 1280, 720, rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)\n",
    "align = rs.align(rs.stream.depth)\n",
    "\n",
    "print(\"[INFO] Starting streaming...\")\n",
    "pipeline.start(config)\n",
    "print(\"[INFO] Camera ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec5ecdd69eee8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<h3  style=\"font-family: 'News Cycle'\">Loading Model</h3>\n",
    "<p style=\"text-align: justify\">Here, a model instance is initiated and a nano YOLOv8 segmentation model is stored. A list of detectable objects is also extracted and printed to the console.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a9c12f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8n-seg.pt\")\n",
    "class_names = model.names\n",
    "print('Class Names: ', class_names)\n",
    "colors = [[random.randint(0, 255) for _ in range(3)] for _ in class_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a860ce",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<h3  style=\"font-family: 'News Cycle'\">Main Loop</h3>\n",
    "<p style=\"text-align: justify\">With the necessary preparations made, an infinite loop can be constructed. Within each iteration, a color and depth frame pair is processed and displayed.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca152940",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "frames = pipeline.wait_for_frames()\n",
    "aligned_frames = align.process(frames)\n",
    "depth_frame = aligned_frames.get_depth_frame()\n",
    "aligned_color_frame = aligned_frames.get_color_frame()\n",
    "color_frame = frames.get_color_frame()\n",
    "if not depth_frame or not aligned_color_frame: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c910e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "color_intrin = aligned_color_frame.profile.as_video_stream_profile().intrinsics\n",
    "depth_image = np.asanyarray(depth_frame.get_data())\n",
    "color_image = np.asanyarray(color_frame.get_data())\n",
    "h, w, _ = color_image.shape\n",
    "results = model.predict(color_image, stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b4a404",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for r in results:\n",
    "        boxes = r.boxes  # Boxes object for bbox outputs\n",
    "        masks = r.masks  # Masks object for segment masks outputs\n",
    "        probs = r.probs  # Class probabilities for classification outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a04dc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "if masks is not None:\n",
    "    masks = masks.data.cpu()\n",
    "    for seg, box in zip(masks.data.cpu().numpy(), boxes):\n",
    "        seg = cv2.resize(seg, (w, h))\n",
    "        color_image = overlay(color_image, seg, colors[int(box.cls)], 0.4)\n",
    "\n",
    "        count = (seg == 1).sum()\n",
    "        x, y = np.argwhere(seg == 1).sum(0) / count\n",
    "        depth = depth_frame.get_distance(int(y), int(x))\n",
    "        dx, dy, dz = rs.rs2_deproject_pixel_to_point(color_intrin, [y, x], depth)\n",
    "        distance = math.sqrt(((dx) ** 2) + ((dy) ** 2) + ((dz) ** 2))\n",
    "\n",
    "        xmin = int(box.data[0][0])\n",
    "        ymin = int(box.data[0][1])\n",
    "        xmax = int(box.data[0][2])\n",
    "        ymax = int(box.data[0][3])\n",
    "\n",
    "        plot_one_box([xmin, ymin, xmax, ymax], color_image, colors[int(box.cls)],\n",
    "                     f'{class_names[int(box.cls)]} {float(box.conf):.3} {float(100*distance):.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a949db8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "cv2.imshow('img', color_image)\n",
    "if cv2.waitKey(1) & 0xFF == ord('q'):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c73c21614c289bf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<h3  style=\"font-family: 'News Cycle'\">Clean-up and Resource Management</h3>\n",
    "<p style=\"text-align: justify\">To free up memory and disengage the camera, the pipeline needs to be stopped at the end of the operation.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3c3f9b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"[INFO] stop streaming ...\")\n",
    "pipeline.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77806173",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"text-align: center; font-family: 'News Cycle'\">Live Demo</h1>\n",
    "<img src=\"img/demo.svg\" alt=\"Live Demo\" align=\"middle\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e27f0a-99db-4af2-957b-9d38f241a253",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h2  style=\"font-family: 'News Cycle'\">Sources</h2>\n",
    "<ol>\n",
    "    <li>Technical information on RealSense Depth Cameras and the image: <a href=\"https://www.intelrealsense.com/depth-camera-d415/\">intelrealsense.com</a></li>\n",
    "    <li>Comparison between Faster RCNN, YOLO, and SSD: <a href=\"https://medium.com/ibm-data-ai/faster-r-cnn-vs-yolo-vs-ssd-object-detection-algorithms-18badb0e02dc\">medium.com</a></li>\n",
    "    <li>YOLOv8 documentation and brand information: <a href=\"https://docs.ultralytics.com/\">ultralytics.com</a></li>\n",
    "    <li>COCO documentation: <a href=\"https://cocodataset.org/\">cocodataset.org/</a></li>\n",
    "    <li>Graphics: <a href=\"https://www.freepik.com\">freepik.com</a></li>\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c023e-64c0-412a-9046-6fffb9fcadd0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"text-align: center; font-family: 'News Cycle'\">Thank you</h1>\n",
    "<img src=\"img/thank.jpg\" alt=\"Thank you\"   style=\"display: block; margin-left: auto; margin-right: auto\"/>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
